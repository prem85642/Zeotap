{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61beb474-2fcf-4b0b-ae16-0803ab0b9d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Features prepared successfully. Shape: (200, 8)\n",
      "Clusters 2: DB Index = 1.5050, Silhouette = 0.2547\n",
      "Clusters 3: DB Index = 1.8440, Silhouette = 0.1741\n",
      "Clusters 4: DB Index = 1.7322, Silhouette = 0.1775\n",
      "Clusters 5: DB Index = 1.5760, Silhouette = 0.1843\n",
      "Clusters 6: DB Index = 1.5633, Silhouette = 0.1772\n",
      "Clusters 7: DB Index = 1.4489, Silhouette = 0.1769\n",
      "Clusters 8: DB Index = 1.4643, Silhouette = 0.1863\n",
      "Clusters 9: DB Index = 1.4352, Silhouette = 0.1862\n",
      "Clusters 10: DB Index = 1.4468, Silhouette = 0.1822\n",
      "Optimal number of clusters: 9\n",
      "Clustering analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CustomerSegmentation:\n",
    "    def __init__(self, customer_file='Customers.csv',\n",
    "                 product_file='Products.csv',\n",
    "                 transaction_file='Transactions.csv'):\n",
    "        \"\"\"Initialize the CustomerSegmentation class with data files\"\"\"\n",
    "        try:\n",
    "            self.customers_df = pd.read_csv(customer_file)\n",
    "            self.products_df = pd.read_csv(product_file)\n",
    "            self.transactions_df = pd.read_csv(transaction_file)\n",
    "\n",
    "            # Initialize other attributes\n",
    "            self.feature_matrix = None\n",
    "            self.scaler = StandardScaler()\n",
    "            self.kmeans = None\n",
    "            self.n_clusters = None\n",
    "            self.features = None\n",
    "            self.cluster_labels = None\n",
    "\n",
    "            print(\"Data loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _validate_data(self):\n",
    "        \"\"\"Validate the loaded data\"\"\"\n",
    "        required_columns = {\n",
    "            'customers': ['CustomerID', 'SignupDate', 'Region'],\n",
    "            'products': ['ProductID', 'Category'],\n",
    "            'transactions': ['TransactionID', 'CustomerID', 'ProductID', 'TransactionDate', 'TotalValue']\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Check required columns\n",
    "            for col in required_columns['customers']:\n",
    "                assert col in self.customers_df.columns\n",
    "            for col in required_columns['products']:\n",
    "                assert col in self.products_df.columns\n",
    "            for col in required_columns['transactions']:\n",
    "                assert col in self.transactions_df.columns\n",
    "\n",
    "            # Check for null values\n",
    "            assert not self.customers_df['CustomerID'].isnull().any()\n",
    "            assert not self.products_df['ProductID'].isnull().any()\n",
    "            assert not self.transactions_df['TransactionID'].isnull().any()\n",
    "\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            print(\"Data validation failed. Please check the required columns and null values.\")\n",
    "            return False\n",
    "\n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare features for clustering with error handling\"\"\"\n",
    "        try:\n",
    "            if not self._validate_data():\n",
    "                raise ValueError(\"Data validation failed\")\n",
    "\n",
    "            # Convert dates safely\n",
    "            self.customers_df['SignupDate'] = pd.to_datetime(self.customers_df['SignupDate'], errors='coerce')\n",
    "            self.transactions_df['TransactionDate'] = pd.to_datetime(self.transactions_df['TransactionDate'], errors='coerce')\n",
    "\n",
    "            # Create customer level features\n",
    "            customer_features = []\n",
    "            current_date = datetime.now()\n",
    "\n",
    "            for customer_id in self.customers_df['CustomerID'].unique():\n",
    "                try:\n",
    "                    # Get customer transactions\n",
    "                    customer_transactions = self.transactions_df[\n",
    "                        self.transactions_df['CustomerID'] == customer_id\n",
    "                    ]\n",
    "                    customer_data = self.customers_df[\n",
    "                        self.customers_df['CustomerID'] == customer_id\n",
    "                    ].iloc[0]\n",
    "\n",
    "                    # Calculate basic features\n",
    "                    features = {\n",
    "                        'customer_id': customer_id,\n",
    "                        'total_spend': customer_transactions['TotalValue'].sum(),\n",
    "                        'avg_transaction_value': customer_transactions['TotalValue'].mean(),\n",
    "                        'num_transactions': len(customer_transactions),\n",
    "                        'num_unique_products': customer_transactions['ProductID'].nunique(),\n",
    "                        'days_since_signup': (current_date - pd.Timestamp(customer_data['SignupDate'])).days,\n",
    "                        'region': customer_data['Region']\n",
    "                    }\n",
    "\n",
    "                    # Add category preferences\n",
    "                    customer_products = customer_transactions.merge(\n",
    "                        self.products_df, on='ProductID', how='left'\n",
    "                    )\n",
    "                    category_spend = customer_products.groupby('Category')['TotalValue'].sum()\n",
    "\n",
    "                    for category in self.products_df['Category'].unique():\n",
    "                        features[f'spend_{category}'] = category_spend.get(category, 0)\n",
    "\n",
    "                    customer_features.append(features)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing customer {customer_id}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            self.features = pd.DataFrame(customer_features)\n",
    "\n",
    "            # Prepare feature matrix for clustering\n",
    "            feature_cols = [col for col in self.features.columns\n",
    "                          if col not in ['customer_id', 'region']\n",
    "                          and not self.features[col].isnull().any()]\n",
    "\n",
    "            # Handle any remaining null values\n",
    "            self.features[feature_cols] = self.features[feature_cols].fillna(0)\n",
    "\n",
    "            # Scale features\n",
    "            self.feature_matrix = self.scaler.fit_transform(self.features[feature_cols])\n",
    "\n",
    "            print(f\"Features prepared successfully. Shape: {self.feature_matrix.shape}\")\n",
    "            return self.features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in prepare_features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def find_optimal_clusters(self, min_clusters=2, max_clusters=10):\n",
    "        \"\"\"Find optimal number of clusters with error handling\"\"\"\n",
    "        try:\n",
    "            if self.feature_matrix is None:\n",
    "                raise ValueError(\"Features not prepared. Run prepare_features first.\")\n",
    "\n",
    "            db_scores = []\n",
    "            silhouette_scores = []\n",
    "\n",
    "            for n in range(min_clusters, max_clusters + 1):\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
    "                    clusters = kmeans.fit_predict(self.feature_matrix)\n",
    "\n",
    "                    db_score = davies_bouldin_score(self.feature_matrix, clusters)\n",
    "                    silhouette_avg = silhouette_score(self.feature_matrix, clusters)\n",
    "\n",
    "                    db_scores.append(db_score)\n",
    "                    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "                    print(f\"Clusters {n}: DB Index = {db_score:.4f}, Silhouette = {silhouette_avg:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating metrics for {n} clusters: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Plot evaluation metrics\n",
    "            plt.figure(figsize=(15, 6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(range(min_clusters, max_clusters + 1), db_scores, marker='o')\n",
    "            plt.title('Davies-Bouldin Index by Number of Clusters')\n",
    "            plt.xlabel('Number of Clusters')\n",
    "            plt.ylabel('DB Index')\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(min_clusters, max_clusters + 1), silhouette_scores, marker='o')\n",
    "            plt.title('Silhouette Score by Number of Clusters')\n",
    "            plt.xlabel('Number of Clusters')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('FirstName_LastName_Clustering_Metrics.png')\n",
    "            plt.close()\n",
    "\n",
    "            # Select optimal number of clusters\n",
    "            self.n_clusters = db_scores.index(min(db_scores)) + min_clusters\n",
    "            print(f\"Optimal number of clusters: {self.n_clusters}\")\n",
    "\n",
    "            return self.n_clusters, min(db_scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in find_optimal_clusters: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def perform_clustering(self):\n",
    "        \"\"\"Perform clustering with the optimal number of clusters\"\"\"\n",
    "        try:\n",
    "            if self.n_clusters is None:\n",
    "                raise ValueError(\"Optimal number of clusters not determined. Run find_optimal_clusters first.\")\n",
    "\n",
    "            # Perform final clustering\n",
    "            self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "            self.cluster_labels = self.kmeans.fit_predict(self.feature_matrix)\n",
    "\n",
    "            # Add cluster labels to features\n",
    "            self.features['Cluster'] = self.cluster_labels\n",
    "\n",
    "            # Generate cluster insights\n",
    "            cluster_insights = []\n",
    "            for cluster in range(self.n_clusters):\n",
    "                cluster_data = self.features[self.features['Cluster'] == cluster]\n",
    "                insight = {\n",
    "                    'Cluster': cluster,\n",
    "                    'Size': len(cluster_data),\n",
    "                    'Avg_Spend': cluster_data['total_spend'].mean(),\n",
    "                    'Avg_Transactions': cluster_data['num_transactions'].mean(),\n",
    "                    'Dominant_Region': cluster_data['region'].mode().iloc[0]\n",
    "                }\n",
    "                cluster_insights.append(insight)\n",
    "\n",
    "            # Save results\n",
    "            cluster_insights_df = pd.DataFrame(cluster_insights)\n",
    "            cluster_insights_df.to_csv('FirstName_LastName_Cluster_Insights.csv', index=False)\n",
    "\n",
    "            # Visualize clusters\n",
    "            self._visualize_clusters()\n",
    "\n",
    "            return self.features, cluster_insights_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in perform_clustering: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _visualize_clusters(self):\n",
    "        \"\"\"Create visualization of clusters\"\"\"\n",
    "        try:\n",
    "            # Create PCA visualization\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=2)\n",
    "            pca_result = pca.fit_transform(self.feature_matrix)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
    "                                c=self.cluster_labels, cmap='viridis')\n",
    "            plt.title('Customer Segments Visualization (PCA)')\n",
    "            plt.xlabel('First Principal Component')\n",
    "            plt.ylabel('Second Principal Component')\n",
    "            plt.colorbar(scatter)\n",
    "            plt.savefig('FirstName_LastName_Clusters_Visual.png')\n",
    "            plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in cluster visualization: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize segmentation\n",
    "        segmentation = CustomerSegmentation()\n",
    "\n",
    "        # Prepare features\n",
    "        segmentation.prepare_features()\n",
    "\n",
    "        # Find optimal number of clusters\n",
    "        n_clusters, db_score = segmentation.find_optimal_clusters()\n",
    "\n",
    "        # Perform clustering\n",
    "        features, insights = segmentation.perform_clustering()\n",
    "\n",
    "        # Save final report\n",
    "        with open('FirstName_LastName_Clustering.pdf', 'w') as f:\n",
    "            f.write(\"Customer Segmentation Analysis Report\\n\\n\")\n",
    "            f.write(f\"Number of Clusters: {n_clusters}\\n\")\n",
    "            f.write(f\"Davies-Bouldin Index: {db_score:.4f}\\n\\n\")\n",
    "            f.write(\"Cluster Insights:\\n\")\n",
    "            f.write(insights.to_string())\n",
    "\n",
    "        print(\"Clustering analysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db9c35-8801-49cf-9e43-173f0346a046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
